<html><head><title>The Global Adaptation Mapping Initiative (GAMI): Part 3 – Coding protocol</title></head><body><h1>The Global Adaptation Mapping Initiative (GAMI): Part 3 – Coding protocol</h1>Authors: Alexandra Lesnikowski, Lea Berrang-Ford, A.R. Siders, Neal Haddaway, Robbert Biesbroek, Sherilee Harper, Jan Minx, Erin Coughlan de Perez, Diana Reckien, Mark New, Chandni Singh, Adelle Thomas, Edmond Totin, Chris Trisos, Bianca Van Bavel<br>Abstract: <p><em>Context</em>: It is now widely accepted that the climate is changing, and that societal response will need to be rapid and comprehensive to prevent the most severe impacts. A key milestone in global climate governance is to assess progress on adaptation. To-date, however, there has been negligible robust, systematic synthesis of progress on adaptation or adaptation-relevant responses globally.&nbsp;</p><p><em>Aim: </em>The purpose of this review protocol is to outline the methods used by the Global Adaptation Mapping Initiative (GAMI) to <strong><em>systematically review human adaptation responses to climate-related changes that have been documented globally since 2013 in the scientific literature</em></strong>.<strong><em> </em></strong>The broad question underpinning this review is: <em>Are we adapting to climate change?</em><strong><em> </em></strong>More specifically, we ask ‘<em>what is the evidence relating to human adaptation-related responses that can (or are) directly reducing risk, exposure, and/or vulnerability to climate change</em>?’&nbsp;</p><p><em>Methods: </em>We review scientific literature 2013-2019 to identify documents empirically reporting on observed adaptation-related responses to climate change in human systems that can directly reduce risk. We exclude non-empirical (theoretical &amp; conceptual) literature and adaptation in natural systems that occurs without human intervention. Included documents were coded across a set of questions focused on: Who is responding? What responses are documented? What is the extent of the adaptation-related response? What is the evidence that&nbsp; adaptation-related responses reduce risk, exposure and/or vulnerability? Once articles are coded, we conduct a quality appraisal of the coding and develop ‘evidence packages’ for regions and sectors. We supplement this systematic mapping with an expert elicitation exercise, undertaken to assess bias and validity of insights from included/coded literature <em>vis a vis </em>perceptions of real-world adaptation for global regions and sectors, with associated confidence assessments.&nbsp;</p><p><em>Related protocols: </em>This protocol represents Part 3 of a 5-part series outlining the phases of this initiative. Part 3 outlines the methods used to extract data on adaptation from documents (coding), as well as procedures for data quality assurance. See Figure 1.</p><br>Keywords:  climate change, adaptation, resilience, resilient, risk management, global warming, systematic review, evidence synthesis, machine learning, climate, stocktake<br><h2>Introduction</h2><p>The Paris Agreement and Katowice Climate Package articulated a clear mandate to document and assess adaptation progress towards the Global Goal on Adaptation. This includes regularly scheduled stocktaking exercises to summarize and synthesis progress on adaptation. The Global Stocktake (GST) thus underpins the global mandate to track collective progress on how human and natural systems are responding to climatic changes. Despite this, there has to-date been negligible systematic assessment or synthesis of adaptation responses globally. There is, however, a proliferation of documents reporting on adaptation-related efforts and experiences across different sectors, systems, and populations. This review seeks to systematically synthesize this growing literature to summarize diverse forms of evidence documenting global adaptation progress across sectors, systems, and populations.&nbsp;</p><p><br></p><p><em>Stakeholder Engagement</em></p><p>This review responds to the mandate of the IPCC’s AR6 outline, which highlights the need to document and synthesize observed responses to climate change (<a href="http://www.ipcc.ch/site/assets/uploads/2018/03/AR6_WGII_outlines_P46.pdf" rel="noopener noreferrer" target="_blank">http://www.ipcc.ch/site/assets/uploads/2018/03/AR6_WGII_outlines_P46.pdf</a> and <a href="http://www.ipcc.ch/site/assets/uploads/2018/09/220520170356-Doc.-2-Chair-Vision-Paper-.pdf" rel="noopener noreferrer" target="_blank">http://www.ipcc.ch/site/assets/uploads/2018/09/220520170356-Doc.-2-Chair-Vision-Paper-.pdf</a>).&nbsp;</p><p><br></p><p>The approved outline of the IPCC’s AR6 Working Group II report reflects an extensive consultatory process that includes climate change experts from across disciplines, users of the IPCC reports, and representatives from governments: (<a href="https://www.ipcc.ch/site/assets/uploads/2018/03/AR6_WGII_outlines_P46.pdf" rel="noopener noreferrer" target="_blank">https://www.ipcc.ch/site/assets/uploads/2018/03/AR6_WGII_outlines_P46.pdf</a>) .&nbsp;</p><p><br></p><p>Throughout this protocol, we draw on the foci, categorization, and priorities outlined in the IPCC AR6 WGII outline as a reflection of stakeholder framing for this review. To maximize potential impact of outputs, the timeline for this review has additionally been aligned with the publication schedule and publication cut-offs to inform the AR6 assessment process (<a href="https://www.ipcc.ch/site/assets/uploads/2018/12/Timeline_WGIIAR6.pdf" rel="noopener noreferrer" target="_blank">https://www.ipcc.ch/site/assets/uploads/2018/12/Timeline_WGIIAR6.pdf</a>)</p><p><br></p><p><em>Reporting standards</em></p><p>This protocol follows guidance for systematic review mapping (e.g. James et al. 2016) and general guidelines for evidence synthesis (Cochrane, Campbell, CEE). We follow the ROSES established reporting standards (Haddaway et al. 2018).&nbsp;</p><p><br></p><p><em>Funding</em></p><p>The Global Adaptation Mapping Initiative has no formal funding, and is supported by a network of researchers around the world who have contributed their in-kind time to this initiative.&nbsp;</p><p><br></p><p><em>Objective of the review</em></p><p>We frame the review using standards for formulating research questions and searches in systematic reviews, using a PICoST approach: population/problem (P), interest (I), context (Co), and Time (T) and Scope (S) (Table 1). &nbsp;</p><p><br></p><p>The activity of interest (I) is adaptation-related responses. Due to the lack of scientifically-robust literature assessing the potential effectiveness of responses, we use the term ‘adaptation-related responses’ rather than the more common ‘adaptations’ to avoid the implication that all responses (or adaptations) are actually adaptive (i.e. reduce vulnerability and/or risk); some responses labelled as ‘adaptations’ might in fact be maladaptive. To be included, responses must be initiated by humans. This includes human-assisted responses within natural systems, as well as responses taken by governments, the private sector, civil society, communities, households, and individuals, whether intentional/planned or unintentional/autonomous. While unintentional/autonomous responses are included, these are likely to be under-represented unless labelled as adaptation and documented as a response to climate change due to the infeasibility of capturing potential adaptive activities not identified as adaptations. We exclude responses in natural systems that are not human-assisted; these are sometimes referred to as evolutionary adaptations or autonomous natural systems adaptations. While important, autonomous adaptation in natural systems is distinct from adaptations initiated by humans; this review focuses on responses by humans to observed or projected climate change risk. We include any human responses to climate change impacts that are, or could, decrease vulnerability or exposure to climate-related hazards, as well as anticipatory measures in response to expected impacts.&nbsp;</p><p><br></p><p>This review focuses on adaptation only, and excludes mitigation (responses involving the reduction of greenhouse gas (GHG) concentrations). We consider adaptation responses across contexts (Co) globally, and focus only on adaptation activities that are directly intended to reduce risk, exposure, or vulnerability, even if later identified as maladaptation. To reflect publications since AR5 and prior to the AR6 publication cut-off, we focus on literature published in the time period (T) between 2013 and 2020.&nbsp;</p><p><br></p><p>This review focuses on the scientific literature only, and excludes grey literature and other sources of Indigenous and Local Knowledge (IKLK).&nbsp;</p><h2>Procedure</h2><p>This protocol represents Part 3 of a 5-part series outlining the phases of methods for this initiative. Part 3 outlines the methods used to extract data on adaptation from documents (coding), as well as procedures for data quality assurance. See Figure 1. Figure 2 provides a summary of the number of articles included and excluded at different phases.</p><p><br></p><p><strong>1.0&nbsp;Scope</strong></p><p>This data extraction protocol describes methods used to code adaptation information from a dataset of scientific articles. The protocol describing the screening and selection of articles in the dataset can be found here: DOI ***. A total of 2032 articles were retrieved from the screening stage and deemed potentially eligible for data extraction.</p><p><br></p><p>The bibliographic information for articles meeting inclusion criteria during screening were imported into the platform SysRev (sysrev.com). Given that initial screening was conducted on title and abstract only, an additional screening step was undertaken during this phase (data extraction) to ensure documents contained sufficient full-text information to extract relevant data. Thus, data extraction included two initial screening questions:&nbsp;</p><p><br></p><p>1. “<em>Is the document relevant according to inclusion/exclusion criteria?” </em>This question was used to exclude books, conference proceedings, and other document formats missed at the initial screening phase, and to verify relevance of borderline inclusion.</p><p><br></p><p><em>2. Is there sufficient information detailed in the full text (a minimum of half a page of content documenting an adaptation-related response). </em>This question was used to screen out documents referring to relevant adaptation responses in their title or abstract, but including no tangible detail or documentation within the article itself.</p><p><br></p><p><strong>2.0&nbsp;Structure of coding teams and platform</strong></p><p>Data extraction was undertaken within the SysRev on-line systematic review application. SysRev is a freeware application designed to allow web-based data systematic extraction from documents. We created an on-line data extraction form within SysRev to enter and curate extracted data. SysRev enables management of multiple coding of documents, identifies inter-coder conflicts, and links to full-text documents for rapid and standardized review.</p><p><br></p><p>Bibliographic information for all documents classified as relevant to inclusion criteria during screening were imported into SysRev. Given the substantial number of documents and global scope of the review, data extraction was undertaken by small teams of researchers based on regional and sector expertise. Papers were assigned to a primary topic OR region. While each document could be coded as relevant to multiple regions and multiple sectors, an individual document was assigned to a single region or sector to facilitate coding within distinct project teams. A total of 13 ‘projects’ were created, reflecting all regions (n=7) and sectors (n=7) listed in the IPCC AR6 WGII chapter outline (Table 2). Asia and Australasia were combined since the latter had a very small volume of literature. Some coders contributed to multiple projects. Documents were independently coded by at least two individuals.</p><p><br></p><p><strong>3.0&nbsp;Coder recruitment</strong></p><p>Coder recruitment focused on global researchers with expertise in climate change adaptation and one or more of the sectoral or regional topics. The majority of coders had a PhD or higher, though highly specialized researchers with lesser degrees were accepted where relevant and for under-represented topics or regions. For regional sectors, the majority of coders are based in that region, or originate from the region. Coder recruitment was based on convenience recruiting, but prioritized global diversity to seek representation by gender, region, and expertise. Recruitment was based on snowballing via team networks and through social media.</p><p><br></p><p>Coders were expected to code a minimum of 50 documents, and were included as a co-author team member if this was achieved, and if their codes passed quality appraisal.</p><p><br></p><p><strong>4.0&nbsp;Training</strong></p><p>We developed an on-line training manual for coders. The training included both contextual information on systematic review methodologies, as well as key details to guide data extraction, including a detailed codebook. Training of coders sought to expose coders to basic concepts of systematic evidence synthesis and assessment of confidence in evidence. The training manual also served to establish a consistent baseline for the concepts, vocabulary, and definitions used within GAMI, recognizing a wide range of often conflicting definitional uses for adaptation concepts. Sections within the training manual included:</p><p>1. About the GAMI initiative</p><p>2. Why systematic review? (including an introduction to elements of systematic review)</p><p>3. Why create an adaptation database?</p><p>4. The IPCC Risk Framework</p><p>5. Scope of the review</p><p>6. What has already been done?</p><p>7. An introduction to coding</p><p>8. Working with SysRev</p><p>9. Coding documents</p><p>10. Assessment of confidence in evidence</p><p>11. Tips for excellent coding</p><p><br></p><p>GAMI training for coders was originally developed in on-line course format using Eliademy, an e-learning platform to share and manage courses. The Eliademy service was discontinued within 2 months of commencing coding, however, so all course materials were converted to a 26-page training manual. A copy of the training manual can be found in the Supplementary Files.</p><p><br></p><p><strong>5.0&nbsp;Typology for data extraction</strong></p><p>Data extraction was guided by an adaptation typology designed to characterize <em>who</em> is responding, <em>what </em>responses are being observed, what is the <em>extent </em>of the adaptation-related response, and are adaptation-related responses <em>reducing vulnerability and/or risk</em>? Coding of regional and sectoral foci within documents allows stratified analyses for individual sectors or regions.&nbsp;</p><p><br></p><p>Questions included both closed/restricted answer questions and open-ended narrative answer questions. The former facilitate quantitative categorical analysis (e.g. descriptive statistics, summarizing studies in ordered tables) and mapping of adaptation (breadth), while the latter facilitate contextual understanding of adaptation and qualitative analysis.</p><p><br></p><p>The data extraction strategy is designed to create a systematic database characterizing adaptation responses that can be used for multiple types of analyses rather than a single objective. Key analytical questions are summarized in Table 3. A detailed codebook for data extraction is included in the Supplementary Files.</p><p><br></p><p><strong>6.0&nbsp;Missing data and outcome reporting bias</strong></p><p>There is likely to be substantial reporting bias given that many activities that reduce vulnerability and risk are not reported or not labelled as adaptations, particularly in the case of autonomous responses to climate risks. Given the conceptual complexity of the adaptation literature, there are currently no feasible options to overcome this reporting bias at the global scale.&nbsp;</p><p><br></p><p>For individual documents, there may be insufficient information to answer a question in the data extraction form. In this case, all coders will be asked to enter ‘no data’ to distinguish absence of evidence (‘no data’) from evidence of absence. Reporting of confidence in evidence and lack of information for key adaptation needs is a key goal of this initiative.</p><p><br></p><p><strong>7.0&nbsp;Assessment of confidence in evidence&nbsp;</strong></p><p>Quality appraisal was undertaken on all documents/studies meeting inclusion criteria, and was part of the assessment of confidence in evidence. Critical appraisal was not used for article inclusion or exclusion since this review includes literature with a range of methods. Appraisal was thus conducted to fulfill the requirements of assessment of confidence in evidence. The appraisal is guided by components of the GRADE-CerQual (<a href="https://www.cerqual.org/a" rel="noopener noreferrer" target="_blank">https://www.cerqual.org/</a>) approach to evaluating confidence in evidence for qualitative data. We did not appraise or extract quantitative data. The following critical appraisal questions were included in the data extraction form:</p><p><strong>A. Are there any major methodological limitations?</strong> E.g. Are methods sufficient to answer the research question, and are findings adequately and sufficiently substantiated by empirical data (qualitative or quantitative data)? Are there any major sources of bias in the data collection, analysis, or interpretation of results? Comments on methodological limitations.</p><p><strong>B. Assessing coherence: Did the article provide sufficient information to answer all of your coding questions?</strong> Were there particular questions for which you felt that there was: 1) limited information or unclear evidence provided, 2) divergent results or outliers that made it hard to answer or that the authors seemed to ignore, or 3) the paper/document was not really directly relevant to the questions you were asking? This question will help us assess confidence in findings. Please highlight any of your answers that may be less reliable compared to others.</p><p><strong>C. Assessing adequacy: Please comment on the quantity and quality of data upon which the findings in this article/document are based</strong> (e.g. sample size and/or depth of research). Did the article/document contain sufficient and adequate data (quantity and/or richness) for you to feel confident answering these questions? This question will help us assess confidence in findings. We are less confident about a finding when the underlying data only come from a small number of participants, locations, or settings, or in the case of case-studies do not contain sufficient detail/richness to make a meaningful assessment.</p><p><strong>D. Assessing relevance: Are the results of this study relevant to a particular context only</strong> (e.g. a particular region, population, or context)? If so, describe the context within which these results are valid/relevant.</p><p><br></p><p><strong>8.0&nbsp;Quality assurance of coding</strong></p><p>To enable cross-article comparisons, it is important that all coders follow the coding guidelines and answer all questions.&nbsp;We therefore conducted a quality assessment for each coder to identify those who had missed entries or skipped significant questions within the SysRev data extraction platform. Sixteen key questions were identified that had closed-option responses and no logical conditions (i.e., were not answered only if a previous question were true). Any coder who left &gt;10% of these key questions blank was asked to complete their codes. Response rates were calculated using R. The code is available on GitHub: <a href="http://doi.org/10.5281/zenodo.4010763" rel="noopener noreferrer" target="_blank">doi.org/10.5281/zenodo.4010763</a>.&nbsp;Any coder who was unable or unwilling to complete their codes was deemed to have unreliable codes. To be included in the database, a document must have at least one set of reliable codes. In cases where a document did not have at least one set of reliable codes, we sought a third coder.</p><p><br></p><p>All coders were contacted at the end of initial coding to ask them to ensure completeness of all codes and to flag key areas of potential error. This included, for example, avoiding blank entries that should instead be listed as ‘not relevant’ or ‘no information’; ensuring that multiple relevant sectors and regions are recorded, regardless of project team; and avoiding exclusion of non-English language articles. Articles assigned to coders without relevant language abilities were reassigned to another coder with appropriate language skills.&nbsp;</p><p><br></p><p><strong>9.0&nbsp;Reconciliation of double codes</strong></p><p>Over 100 volunteers coded more than 2,500 articles. 482 articles were excluded (book chapters, not human adaptation, etc.). At least two volunteers coded 2,177 articles (the remaining 16 articles were coded by a single reliable coder). To consolidate multiple responses into a single entry for each article, we used a script in R that followed a series of if/then statements. The full code and rationale are available on GitHub (<a href="http://doi.org/10.5281/zenodo.4010763" rel="noopener noreferrer" target="_blank">doi.org/10.5281/zenodo.4010763</a>). For open-ended questions that asked coders to provide quotes or evidence, all responses were compiled. For True/False questions, if either coder responded True, the answer was coded as True because these questions ask about the absence or presence of certain topics in each article, and it is more likely that one coder overlooked the presence of an item (gave a false negative) than that a coder imagined the presence of something not actually present (gave a false positive). For questions with multiple responses (e.g., hazards addressed), similar logic led us to take all responses because false negatives were more likely than false positives. In all cases, decisions were made to be conservative - to overestimate the degree and amount of adaptation being documented. Reconciliation stages were systematically biased to include rather than exclude, so as to retain the most detail possible.&nbsp;</p><p><br></p><p>A final database was compiled with a single line entry for each article. Authors and title names were used to double-check for duplicates within the database (duplicate entries were merged). And articles were assigned to IPCC regions based on the countries identified during coding. In most cases, these aligned with the GAMI-assigned regions, but some island states, for example, were assigned to different regions, and a few errors in regional assignment were corrected. The final database contains 1682 articles and 70 columns (70 data points for each article).&nbsp;</p><p><br></p><h2>Time Taken</h2><p>The full GAMI work, including all stages, was undertaken over a period of approximately 12-18 months (2019-20).</p><h2>Anticipated Results</h2><p>The results of this initiative comprise a database and a set of evidence packages documenting key insights from scientific literature documenting global human adaptation to climate change. These data have been provided to author teams leading the Intergovernmental Panel on Climate Change (IPCC) 6th Assessment Report (AR6), Working Group II, to support their climate assessments. The database is also the basis for a number of secondary analyses and publications, focusing on particular regions, sectors, or aspects of adaptation. Publications are forthcoming.</p><h2>References</h2><p>Siders, A.R. (2020, August 1). GAMI Intercoder Reliability &amp; Reconciliation. Zenodo. <a href="http://doi.org/10.5281/zenodo.4010762" rel="noopener noreferrer" target="_blank">http://doi.org/10.5281/zenodo.4010763</a></p></body></html>