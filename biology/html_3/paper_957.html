<article>
<section>
<h1 id="header">A behavioral protocol to assess the relationship between three cognitive biases and future depression severity</h1>
<p><time datetime="2021-08-27">Published: August 27, 2021</time></p>
<p>Pengyu Zhang,<sup><a href="#aff1">1</a>,<a href="#fn1">8</a></sup> Yi Piao,<sup><a href="#aff2">2</a>,<a href="#aff3">3</a></sup> Ying Chen,<sup><a href="#aff2">2</a></sup> Jiecheng Ren,<sup><a href="#aff1">1</a></sup> Longhua Zhang,<sup><a href="#aff1">1</a></sup> Bensheng Qiu,<sup><a href="#aff4">4</a></sup> Zhengde Wei,<sup><a href="#aff1">1</a>,<a href="#aff5">5</a>,<a href="#cor1">*</a></sup> and Xiaochu Zhang<sup><a href="#aff1">1</a>,<a href="#aff2">2</a>,<a href="#aff6">6</a>,<a href="#aff7">7</a>,<a href="#fn2">9</a>,<a href="#cor2">**</a></sup></p>
<p id="aff1"><sup>1</sup>Department of Radiology, the First Affiliated Hospital of USTC, Hefei National Laboratory for Physical Sciences at the Microscale and School of Life Science, Division of Life Science and Medicine, University of Science &amp; Technology of China, Hefei 230027, China</p>
<p id="aff2"><sup>2</sup>Department of Psychology, School of Humanities &amp; Social Science, University of Science &amp; Technology of China, Hefei, Anhui 230026, China</p>
<p id="aff3"><sup>3</sup>Institute of Advanced Technology, University of Science and Technology of China, Hefei, Anhui 230026, China</p>
<p id="aff4"><sup>4</sup>Centers for Biomedical Engineering, University of Science &amp; Technology of China, Hefei, Anhui 230027, China</p>
<p id="aff5"><sup>5</sup>Shanghai Key Laboratory of Psychotic Disorders, Shanghai Mental Health Center, Shanghai Jiao Tong University School of Medicine, Shanghai 200030, China</p>
<p id="aff6"><sup>6</sup>Hefei Medical Research Center on Alcohol Addiction, Affiliated Psychological Hospital of Anhui Medical University, Hefei Fourth People’s Hospital, Anhui Mental Health Center, Hefei 230017, China</p>
<p id="aff7"><sup>7</sup>Academy of Psychology and Behavior, Tianjin Normal University, Tianjin 300387, China</p>
<p id="fn1"><sup>8</sup>Technical contact</p>
<p id="fn2"><sup>9</sup>Lead contact</p>
<p id="cor1"><sup>*</sup>Correspondence: <a href="mailto:zdwei@mail.ustc.edu.cn">zdwei@mail.ustc.edu.cn</a></p>
<p id="cor2"><sup>**</sup>Correspondence: <a href="mailto:zxcustc@ustc.edu.cn">zxcustc@ustc.edu.cn</a></p>
<p><span class="open-access">Open Access</span> • DOI: <a href="https://doi.org/10.1016/j.xpro.2021.100773">10.1016/j.xpro.2021.100773</a></p>
</section>
<section>
<h2 id="summary">Summary</h2>
<p>According to the cognitive model of depression, memory bias, interpretation bias, and attention bias are associated with the development and maintenance of depression. Here, we present a protocol for investigating whether and how the novel coronavirus disease 2019 (COVID-19) pandemic may affect the relationship between current cognitive biases and future depression severity in a population with non-clinical depression. This protocol can also be used in other contexts, including cognitive bias-related studies and depression-related functional magnetic resonance imaging (fMRI) studies.</p>
<p>For complete details on the use and execution of this protocol, please refer to <a href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref9">Zhang et al. (2021)</a>.</p>




<div class="highlights">
<h3>Highlights</h3>
<ul>
<li>A protocol to assess relationship between cognitive biases and depression severity</li>
<li>Guidelines for evaluating memory, interpretation, and attention biases</li>
<li>Guidelines for conducting an fMRI experiment</li>
<li>Effect of COVID-19 on relationship between memory bias and depression severity</li>
</ul>
</div>
<div class="graphical-abstract">
<h3>Graphical abstract</h3>
<figure><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-GA.jpg" alt="GraphicalAbstract.jpg"></figure>
</div></section>
<section>
<h2 id="before-you-begin">Before you begin</h2>
<p>The Human Research Ethics Committee of the University of Science and Technology of China approved this study. This protocol can also be used to investigate the effect of other factors on the relationship between current cognitive biases and future depression severity.</p>
<h3 id="sec1.1">Prepare the experimental materials</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 10 h</div>
<ol>
<li>Prepare an online Beck Depression Inventory-II (BDI-II) questionnaire and an online socializing questionnaire (<a href="#bib2">Beck et al., 1996</a>).
<ol type="a">
<li>The online BDI questionnaire will be used in the participant screening session and follow-up sessions. The socializing questionnaire will be used in the socializing information collection session.</li>
<li>The questionnaires should be made as online versions, so that the participants can complete them online and don’t need to come back to the lab during the follow-up sessions.</li>
<li>The questions used in the socializing questionnaire are listed in step 21.</li>
</ol>
</li>
<li>The website used to make the online questionnaires in the protocol is <a href="https://www.wjx.cn/">https://www.wjx.cn/</a>.Prepare word images for the task assessing memory bias.
<ol type="a">
<li>Prepare 48 neutral word images for the exercise phase.</li>
<li>Prepare 32 neutral word images, 32 negative word images and 32 positive word images for the priming phase and word identification phase.</li>
<li>Prepare a mask image with random gray value at each pixel. The size of the mask is the same as the word image.</li>
<li>The size of each word image that we used for the experiment is 297×158 pixels.</li>
<li>The color of the word is black and the background is white.</li>
<li>The words used in this protocol are from the materials of a previous study (<a href="#bib6">Tarsia et al., 2003</a>) and translated into Chinese.</li>
<li>To use the provided task programs (<a href="https://github.com/JunbaoZhang/CognitiveBias">https://github.com/JunbaoZhang/CognitiveBias</a>), the file names and the path should follow the provided examples.</li>
</ol>
</li>
<li>Prepare sentence images for the task assessing interpretation bias.
<ol type="a">
<li>Prepare 24 sentence images, with one sentence depicting an ambiguous scenario in each image.</li>
<li>The size of each sentence image that we used for the experiment is 1275×135 pixels.</li>
<li>The color of the sentence is white and the background is black.</li>
<li>The sentences used in this protocol are from the materials of a previous study (<a href="#bib3">Berna et al., 2011</a>) and translated into Chinese.</li>
<li>To use the provided task programs (<a href="https://github.com/JunbaoZhang/CognitiveBias">https://github.com/JunbaoZhang/CognitiveBias</a>), the file names and the path should follow the provided examples.</li>
</ol>
</li>
<li>Prepare face images and the emotional face viewing fMRI task for the task assessing attention bias.
<ol type="a">
<li>Prepare 4 sad face images and 4 neutral face images for the task assessing attention bias.</li>
<li>Prepare 20 sad face images, 20 neutral face images and 20 happy face images for the emotional face viewing task during the fMRI scan.</li>
<li>The size of each face image that we used for the experiment is 260×300 pixels.</li>
<li>The gender of the faces should be balanced to avoid the effect of gender.</li>
<li>Convert all images to grayscale and adjust them to have the same mean brightness value to minimize physical differences between categories.</li>
<li>The faces image used in this protocol are selected from the Chinese affective picture system (<a href="#bib1">Bai et al., 2005</a>).</li>
<li>To use the provided task programs (<a href="https://github.com/JunbaoZhang/CognitiveBias">https://github.com/JunbaoZhang/CognitiveBias</a>), the file names and the path should follow the provided examples.</li>
</ol>
</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> Conventional emotional materials are used as stimuli in this protocol. Future researchers may consider using their own materials to capture event-specific cognitive bias based on their own study purpose.</div>
<ol start="5">
<li>Prepare related paper documents.
<ol type="a">
<li>Prepare consent forms. The form should include a brief description of the study, list the magnetic resonance imaging contraindications, and clarify any risks or benefits for participants.</li>
<li>Prepare participants’ information sheet. The sheet should include participants’ age, gender, education level, visual acuity, handedness, alcohol or drug abuse history and contact details (for follow-up sessions).</li>
<li>Prepare safety screening forms for fMRI scan. The form should list the magnetic resonance imaging contraindications and objects that can’t be brought into the MRI scanning room.</li>
</ol>
</li>
</ol>
<h3 id="sec1.2">Participants screening</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 30 min (for 1 participant)</div>
<ol start="6">
<li>Participants complete the consent form to ensure that they understand the motivation of the study and any risks or benefits associated with it (∼ 5 min).</li>
<li>Participants complete a personal information sheet (including questions related to age, education level, gender, visual acuity etc.). Participants should be right-handed, have normal or corrected-to-normal vision, have no magnetic resonance imaging contraindications and have no alcohol or drug abuse history (∼ 10 min).</li>
<li>Assess the participant’s present depression severity using the BDI-II questionnaire and group them into the healthy group (HG, BDI-II cut off range: 0–13) or the nonclinical depressed group (NDG, BDI-II cut off range: 14–63) based on their current BDI score (∼ 10 min). <a href="#troubleshooting">Troubleshooting 1</a>
</li>
<li>Make an appointment with the participant for the date of the experiment (∼ 5 min). The date of the experiment should be no more than one week after the participants screening session. Tell the participant to have no less than 7 h of sleep the night before the experiment. Drinks containing caffeine or alcohol are not allowed on the day of the experiment. Tell female participants to schedule their participation in the experiment when they are not menstruating.</li>
</ol>
</section>
<section>
<h2 id="key-resources-table">Key resources table</h2>
<table id="krt">
<thead>
<tr>
<th>REAGENT or RESOURCE</th>
<th>SOURCE</th>
<th>IDENTIFIER</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Deposited data</td>
</tr>
<tr>
<td>Chinese affective picture system</td>
<td><a href="#bib1">Bai et al. (2005)</a></td>
<td>N/A</td>
</tr>
<tr>
<td>MATLAB code for the task programs</td>
<td>This paper</td>
<td><a href="https://github.com/JunbaoZhang/CognitiveBias">https://github.com/JunbaoZhang/CognitiveBias</a></td>
</tr>
<tr>
<td colspan="3">Experimental models: Organisms/strains</td>
</tr>
<tr>
<td>Human subjects (95 Chinese participants, 40 women, age range: 18–26 years)</td>
<td><a href="#bib9">Zhang et al. (2021)</a></td>
<td>N/A</td>
</tr>
<tr>
<td colspan="3">Software and algorithms</td>
</tr>
<tr>
<td>MATLAB 2014a</td>
<td>MathWorks</td>
<td><a href="https://www.mathworks.com/">https://www.mathworks.com/</a></td>
</tr>
<tr>
<td>Psychtoolbox v3.0.17</td>
<td><a href="#bib4">Kleiner et al. (2007)</a></td>
<td><a href="http://psychtoolbox.org/">http://psychtoolbox.org/</a></td>
</tr>
<tr>
<td>Statistical Parametric Mapping (SPM 12)</td>
<td>The Wellcome Centre for Human Neuroimaging, UCL Queen Square Institute of Neurology</td>
<td><a href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</a></td>
</tr>
<tr>
<td>A toolbox for Data Processing &amp; Analysis of Brain Imaging (DPABI V3.0_171210)</td>
<td><a href="#bib8">Yan et al. (2016)</a></td>
<td><a href="http://rfmri.org/dpabi">http://rfmri.org/dpabi</a></td>
</tr>
<tr>
<td>WFU pick atlas tool v3.0.5</td>
<td><a href="#bib5">Maldjian et al. (2003)</a></td>
<td><a href="https://www.nitrc.org/projects/wfu_pickatlas">https://www.nitrc.org/projects/wfu_pickatlas</a></td>
</tr>
<tr>
<td colspan="3">Other</td>
</tr>
<tr>
<td>3.0 T GE scanner</td>
<td>General Electric Company</td>
<td>N/A</td>
</tr>
</tbody>
</table>
</section>
<section>
<h2 id="step-by-step-method-details">Step-by-step method details</h2>
<h3 id="sec2.1">Assessment of memory bias (day 1)</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 40 min (for 1 participant)</div>
<p>Memory bias is assessed using a word identification task (WI) plus an incidental free recall task (IFR) (<a href="#bib6">Tarsia et al., 2003</a>). This task is comprised of four phases: exercise phase, priming phase, WI phase and IFR phase (<a href="#fig1">Figure 1</a>).</p>
<figure id="fig1"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-Fig1.jpg" alt="Fig1.jpg">
<figcaption>
<div class="figcaption-title">Figure 1. Task for assessing memory bias</div>
<p>This task is comprised of four phases: exercise phase, priming phase, WI phase and IFR phase. 16 blocks of three trials are included in the exercise phase. The words in the three trials of a single block are presented for a different duration (t1, t2, t3), which are preset by the experimenter at the beginning of the exercise task. In the WI phase, the word in each trial is presented for a fixed duration, which is the estimated threshold duration in the exercise phase.</p>
</figcaption>
</figure>
<ol>
<li>Exercise phase (The purpose of this phase is to familiarize participants with the WI phase and estimate each participant’s reading threshold. The outcome of this phase is used to set up the exposure duration of the word stimuli for the WI phase to avoid floor or ceiling effect).
<ol type="a">
<li>Let the participant sit in front of a computer screen. Show the participant the instruction slide and read the following instructions to the participant: “A list of words will be presented one at a time in the center of the screen and the exposure duration of each word is very short. You are invited to guess all of the words presented and read them aloud even if you are not sure or did not believe that you had actually seen a word.”</li>
<li>Start the exercise program and set the duration (number of frames) of the word images.
<ol type="i">
<li>A total of 48 neutral words are presented in pseudorandom order in 16 blocks of three trials in the exercise program.</li>
<li>For each block, the words in the three trials are presented for a different duration, which are preset by the experimenter at the beginning of the exercise task.</li>
<li>In each of the 48 trials, a word is preceded by a mask displayed for 1000 ms and followed by a mask with a variable duration. The total duration of a word and the mask following it is 3000 ms. Each trial is preceded by a fixation cross displayed in the center of the screen for 1000 ms.</li>
<li>As the participants read the words aloud, the experimenter checks the accuracy of their answers.</li>
</ol>
</li>
<li>Check the accuracy of the participant’s answers at preset durations of the words. Record the corresponding duration of the word when the accuracy is approximately 50%. If the accuracy is too high or too low, the duration should be reset, and the exercise phase should be repeated.</li>
</ol>
</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> Computers with higher refresh rates will make it easier for the experimenter to evaluate the reading threshold for each participant, considering that the accuracy can change dramatically when tested near the threshold. Based on a preliminary, we found that it is hard to evaluate the threshold duration on a computer with refresh rate of 60 Hz. The refresh rate of the computer we used in this task was 120 Hz, and the initial preset number of frames for each block in the exercise phase was 7, 8 and 9.</div>
<ol start="2">
<li>Priming phase (The purpose of this phase is to present participants with priming words. Participants are instructed to attentively watch these words and rate them for their emotionality. Participants don’t know they will be asked to recall these words. ).
<ol type="a">
<li>Show the participant the instruction slide and read the following instructions to the participant: “Pay attention to the words appearing one at a time in the center of the screen and rate them for their negativeness or positiveness on a five-point scale that appeared on the screen after each word.”</li>
<li>Start the priming program.
<ol type="i">
<li>In each of 48 trials, a word is presented in the center of the screen for 4000 ms, followed by a five-point rating scale (1 = very negative, 5 = very positive). The rating scale remained on the screen until the participant pressed the select key on the keyboard. The key that pressed by the participant is recorded by the program.</li>
<li>Three types of words (negative, positive, neutral), with 16 words of each type, are presented in a pseudorandom order to the participant.</li>
</ol>
</li>
</ol>
</li>
<li>WI phase (The purpose of this phase is to assess participants’ implicit memory bias.). <a href="#troubleshooting">Troubleshooting 2</a>
<ol type="a">
<li>Show the participant the instruction slide and read the following instructions to the participant: “A list of words will be presented one at a time in the center of the screen and the exposure duration of each word is very short. You are invited to guess all of the words presented and read them aloud even if you are not sure or did not believe that you had actually seen a word.”</li>
<li>Turn on a recording device to record the participant’s answers during the task.</li>
<li>Start the WI program and set the exposure duration of each word as the threshold duration recorded in the exercise phase.
<ol type="i">
<li>The WI phase is comprised of 96 trials. In each trial, word is presented as in the exercise phase, except for the exposure duration, which is constant and set as the threshold duration recorded in the exercise phase to avoid floor or ceiling effects.</li>
<li>Three types of words (negative, positive, neutral), with 32 words of each type, are presented in a pseudorandom order in this phase. Half of the words are the same as those in the priming phase. And the other half are new words.</li>
<li>As the participants read the words aloud, the experimenter checks the accuracy of their answers.</li>
</ol>
</li>
<li>Turn off the recording device and save the data.</li>
</ol>
</li>
<li>IFR phase (The purpose of this phase is to measure participants’ incidental free recall for the priming words. Performance in this phase is used as indices of explicit memory bias.). <a href="#troubleshooting">Troubleshooting 3</a>
<ol type="a">
<li>Immediately after the end of the WI phase, ask the participant to recall the words they have encountered in the previous 3 phases, as many as possible, and write them down.</li>
<li>Ask the participant to recall for at least 8 min.</li>
</ol>
</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> Before the IFR phase, the participants are not supposed to know they will be asked to recall the words encountered in the previous 3 phases. The IFR phase should be started immediately after the end of WI phase.</div>
<div class="note">
<span class="note-title">Note:</span> Although the aim of the IFR phase is to investigate the participants’ memory of the words encountered in the priming phase, the participant is asked to recall the words they have encountered in all of the previous 3 phases. This is to avoid false negatives (words not reported because they were falsely not attributed to the priming phase) (<a href="#bib6">Tarsia et al., 2003</a>).</div>
<div class="note">
<span class="note-title">Note:</span> The WI and IFR phases measure implicit and explicit memory biases, respectively (<a href="#bib6">Tarsia et al., 2003</a>). Since we are only interested in explicit bias in this protocol, the data from the WI phase were not analyzed. Future researchers may consider utilizing the data from the WI phase if they are interested in implicit memory bias.</div>
<div class="pause-point">
<span class="pause-point-title">Pause point:</span> Participants could take a break after finishing the IFR task.</div>
<h3 id="sec2.2">Assessment of interpretation bias (day 1)</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 20 min (for 1 participant)</div>
<p>Interpretation bias is assessed using the ambiguous scenarios test for depressed mood (AST-D, <a href="#fig2">Figure 2</a>) (<a href="#bib3">Berna et al., 2011</a>). Participants are presented with several ambiguous scenarios and asked to rate the pleasantness of each scenario. The rating scores are used to evaluate the interpretation bias of participants.</p>
<figure id="fig2"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-Fig2.jpg" alt="Fig2.jpg">
<figcaption>
<div class="figcaption-title">Figure 2. Task for assessing interpretation bias</div>
<p>The AST-D task is comprised of 24 trials. The procedure of a single trial is shown in the figure.</p>
</figcaption>
</figure>
<ol start="5">
<li>Show participants the instruction slide and read the following instructions to the participant: “You will be presented with a list of scenarios. Form a mental image of each of the scenarios. Imagine each scenario happening to you personally. Follow the first image that comes to mind and then rate how pleasant your image is, as well as how vivid it is.”</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> Participants should be instructed to follow the first image that comes to mind and don’t think too much about each scenario.</div>
<ol start="6">
<li>Start the AST-D program.
<ol type="a">
<li>A total of 24 ambiguous scenarios are presented in the AST-D task.</li>
<li>In each of the 24 trials, participants are presented with a sentence depicting an ambiguous scenario on the screen. Participants imagine the scenario happening to themselves. After the imagination is complete, the participant presses the blank key to go to the next step.</li>
<li>Once participants press the blank key, a 7-point pleasantness rating scale (1 = extremely unpleasant, 7 = extremely pleasant) will be shown on the screen. The rating scale remains on the screen until participants press the select number key on the keyboard. The key that pressed by the participants is recorded by the program.</li>
<li>Once participants rate for the pleasantness, a 7-point vividness rating scale (1 = not vivid at all, 7 = extremely vivid) will be shown on the center of the screen. The rating scale remains on the screen until participants press the select number key on the keyboard. The key that pressed by participants is recorded by the program. Then, the next trial begins.</li>
</ol>
</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> The ambiguous scenarios used in the AST-D task should match the life experience of the participants. The scenarios we used in this protocol was translated from the materials of a previous study (<a href="#bib3">Berna et al., 2011</a>), because the participants’ age in that study (mean age = 22.49 years) is similar with the participants’ age in our study (mean age = 21.22 years). Researchers using this protocol should consider using other ambiguous scenarios if the participants in their study are children or other special populations.</div>
<div class="pause-point">
<span class="pause-point-title">Pause point:</span> Participants could take a break after finishing the AST-D task.</div>
<h3 id="sec2.3">Assessment of attention bias (day 1)</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 30 min (for 1 participant)</div>
<p>Attention bias is assessed using a visual search task (VST, <a href="#fig3">Figure 3</a>) (<a href="#bib7">Wieser et al., 2018</a>). Participants are presented with six faces at each trial and need to detect if the emotion of these faces are the same. The accuracy and reaction time in detecting a sad face among neutral faces can be used as indices of negative attention bias.</p>
<figure id="fig3"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-Fig3.jpg" alt="Fig3.jpg">
<figcaption>
<div class="figcaption-title">Figure 3. Task for assessing attention bias</div>
<p>The VS task is comprised of 252 trials. The procedure of a single trial is shown in the figure.</p>
</figcaption>
</figure>
<ol start="7">
<li>Show the participant the instruction slide and read the following instructions to the participant: “In each trial, six faces arranged in a circle around a fixation cross will be presented on the screen. Please attentively watch the displays on the screen and detect as quickly and accurately as possible a discrepant face in the presented search arrays of six faces. Please press different keys (“yes” or “no” button) depending on whether a discrepant target is present in the array. ”</li>
<li>Before starting the VST task, provide participants with some practice trials.
<ol type="a">
<li>The purpose of the practice trials is to familiarize participants with the rules of VST task and the position of “yes” and “no” buttons.</li>
<li>The practice grogram consists of 7 trials with displays containing a target or not.</li>
<li>The practice trials are randomly intercepted from the trials of VST program.</li>
</ol>
</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> Participants should be instructed to response as quickly and accurately as possible. Besides, make sure participants don’t confuse the “yes” and “no” buttons. Based on our test, some participants may confuse the two buttons in practice trials. If that happens, provide the participants more practice before starting the VST program.</div>
<ol start="9">
<li>Start the VST program. <a href="#troubleshooting">Troubleshooting 4</a>
<ol type="a">
<li>The VST program consists of two blocks, with 126 trials in each block. A 30-s break is provided between two blocks.</li>
<li>Faces from four men and four women (sad, neutral) selected from the Chinese affective picture system (<a href="#bib1">Bai et al., 2005</a>) are used as stimuli in the VST program.</li>
<li>Visual search arrays contain 6 faces with sad targets among neutral distractors, neutral targets among sad distractors, all neutral distractors and all sad distractors (4 conditions). The six faces in each trial are from the same person.</li>
<li>Before each trial, participants are presented with a fixation cross in the center of the screen for 1000 ms. Then six faces arranged in a circle around the fixation cross are presented on the screen. Participants need to detect a discrepant face in the presented six faces as quickly and accurately as possible and press different keys (a “yes” or “no” button) depending on whether a discrepant target is present among the six faces. Once a “yes” or “no” button is pressed, participants will be presented with a blank screen for a random duration (1500, 2000 or 2500 ms). Then the next trial begins.</li>
</ol>
</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> Only sad and neutral faces are used as stimuli in the VST task in this protocol since our primary interest is negative memory bias in <a href="#bib9">Zhang et al. (2021)</a>. Future researchers using this protocol may consider including other kinds of emotional faces (for example, happy faces) based on their own study purpose.</div>
<div class="note">
<span class="note-title">Note:</span> The VST lasts a long time. Thus some participants may not be able to keep their fingers on the keyboard during the whole time of the task. For example, their fingers may leave the keyboard to scratch. To avoid the effect of this issue, there is a 30-s break at the middle of the task. And the experimenter need to emphasize to participants the importance of keeping their fingers on the keyboard. Besides, the experimenter need to pay attention to participants’ performance during the task and label the trials when participants’ fingers leave the keyboard. The labeled trials should be deleted from the analysis.</div>
<div class="pause-point">
<span class="pause-point-title">Pause point:</span> Participants could take a break after finishing the VST.</div>
<h3 id="sec2.4">Acquire fMRI scanning data (day 1)</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 30 min (for 1 participant)</div>
<p>Participants complete an emotional face viewing task during the fMRI scan (<a href="#fig4">Figure 4</a>). The purpose of this session is to detect participants’ brain activation when watch different emotional stimuli.</p>
<figure id="fig4"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-Fig4.jpg" alt="Fig4.jpg">
<figcaption>
<div class="figcaption-title">Figure 4. Emotional face viewing task during fMRI scan</div>
<p>The fMRI experiment is a face viewing task with 2 runs, each with 6 blocks. In each block, 5 trials of male faces and 5 trials of female faces are shown. In each trial, a face is shown in the center of the screen for 200 ms, followed by a black screen. Each trial lasts for 2 s. Then, the next face appears. A fixation cross is presented for 20 s between two blocks and for 10 s before the first block and after the last block of each session.</p>
</figcaption>
</figure>
<ol start="10">
<li>Participants complete a safety screening form for fMRI scan. Participants with MRI contraindications should be excluded from the fMRI experiment.</li>
<li>Check the participant’s visual acuity.
<ol type="a">
<li>Provide the participant, who has a poor visual acuity, with a pair of corrective glasses that is compatible with the MRI scanner.</li>
</ol>
</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> Step 11 (check the participant’s visual acuity) should be completed at the beginning of the fMRI scanning session, so that there are some time for the participant to adapt to the corrective glasses.</div>
<ol start="12">
<li>Read the following instructions to the participant:
<ol type="a">
<li>You will need to lie in a MRI scanner for about 20 min. Please keep your head still when you are in the scanner.</li>
<li>You will also need to complete an emotional face viewing task during the fMRI scan. Please attentively watch the faces presented on the screen during the task.</li>
<li>Please remove all metallic objects from your body (for example, glasses, coins, phones, keys, etc.) before get into the scanner room.</li>
</ol>
</li>
<li>Open the stimulus system.
<ol type="a">
<li>Turn on the stimulus computer and check the resolution and frame rate are set appropriately. In this protocol, the resolution of the stimulus computer is 1024∗768 and the frame rate is 60 Hz.</li>
<li>Turn on the projector.</li>
<li>Open the projector control software on the stimulus computer and check the content on the screen of the stimulus computer is correctly projected by the projector.</li>
<li>Adjust the position of the projector, so that all of the content on the screen of the stimulus computer can be projected on the MRI screen appropriately.</li>
<li>Make sure scanner triggers and button box responses are registered by the stimulus computer.</li>
<li>Open the MATLAB software and the emotional face viewing program on the stimulus computer.</li>
</ol>
</li>
<li>Before getting into the scanner room, ask the participant to change into MRI appropriate slippers and remove all metallic objects from their body. The experimenter also needs to remove all metallic objects from the body. Ask the participant to stuff up the ears with earplugs.</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> Metallic objects can’t be brought into the scanning room. Otherwise, they will affect MRI signals and even cause damage to the scanner.</div>
<ol start="15">
<li>After getting into the scanner room, ask the participant to lie on the scanner table in a comfortable position and keep their head in the head coil (an 8-channel phased array head coil is used in this protocol). Stabilize the participant’s head via foam padding within the head coil. Remind participants not to cross their hands or legs and keep their head still during the scan. Explain to participants that they may tell the experimenter to stop the scan at any time if they feel uncomfortable.</li>
<li>Adjust the mirror system.
<ol type="a">
<li>Make sure the participant can have a full view of the screen projected by the projector.</li>
<li>If the participant cannot see enough of the screen, ask them to adjust the position and angle of the mirror on the top of their head.</li>
</ol>
</li>
<li>Put the button box into the participant’s hand and tell them the position of the buttons.</li>
<li>Move the table into the scanner bore slowly. Then, the experimenter gets out of the scanner room.</li>
<li>Start the scanning session. <a href="#troubleshooting">Troubleshooting 5</a>
<ol type="a">
<li>Register the participant in the MRI scanning control system.</li>
<li>Three-plane localization scan. Check that the head is in the right position.</li>
<li>ASSET Calibration. A calibration scan using array spatial sensitivity encoding technique (ASSET).</li>
<li>Read the following instructions to the participant: Please keep your head still during the scan. If you feel uncomfortable or can’t see the screen in the mirror, please press the “1” button now. Otherwise, please press the “2” button.</li>
<li>If the participant presses the “1” button, stop the scanning session and check what the problem is. If the participant press the “2” button, continue with the scan.</li>
<li>Acquire functional images using T2-weighted sequence (slice number = 36; slice thickness = 3 mm; repetition time [TR] = 2 s; echo time [TE] = 30 ms; flip angle [FA] = 90°; matrix = 64 × 64) while participants performing the face viewing task (<a href="#fig4">Figure 4</a>). The total functional scan is composed of 2 runs, each with 6 blocks. The facial stimuli in each run consists of grayscale normalized sad, happy, and neutral expressions of 30 men and 30 women faces. In each block, 5 trials of male faces and 5 trials of female faces are shown. The expressions of the faces are the same in a single block and different between two adjacent blocks. In each trial, a face is shown in the center of the screen for 200 ms, followed by a black screen. Each trial lasts for 2 s. Then, the next face appears. A fixation cross is presented for 20 s between two blocks and for 10 s before the first block and after the last block of each run. <a href="#troubleshooting">Troubleshooting 6</a>
</li>
<li>Acquire anatomical images using T1-weighted sequence (slice number = 188, slice thickness = 1 mm, repetition time [TR] = 8.2 ms, echo time [TE] = 3.2 ms, slice orientation: sagittal).</li>
<li>Save all the data acquired.</li>
</ol>
</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> The experimenter need to constantly remind the participant to stay as still as possible in the scanner. Head movement may cause distortions that cannot be corrected.</div>
<h3 id="sec2.5">Assessment of future depressive indices (3 months after fMRI scanning)</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 5 min (for 1 participant)</div>
<p>Future depressive indices are assessed three months later using internet-based BDI-II questionnaires.</p>
<ol start="20">
<li>Send the participant a BDI-II questionnaire through the Internet (for example, through WeChat, email, etc.) 3 months later, and ask the participant to complete it.</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> Some of the nonclinical depressed participants’ future depressive indices were assessed during the pandemic, and the others were assessed before the pandemic, as reported in <a href="#bib9">Zhang et al. (2021)</a>. This provided us with the opportunity to investigate the effect of the COVID-19 pandemic on the relationship between cognitive bias and depression severity of nonclinical depression. Members of the NDG whose future BDI-II questionnaires were collected before the outbreak were further classified as the before the outbreak group (BG), and those whose future BDI-II questionnaires were collected during the outbreak were further classified as during the outbreak group (DG).</div>
<h3 id="sec2.6">Socializing information collection (in June 2020)</h3>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 5 min (for 1 participant)</div>
<p>Internet-based questionnaires about socializing before and during COVID-19 were collected in June 2020 in <a href="#bib9">Zhang et al. (2021)</a>.</p>
<ol start="21">
<li>Send participants a questionnaire about socializing before and during the COVID-19 pandemic through the Internet in June 2020 and ask them to complete it.</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> The questionnaire used in this step is provided in the supplemental information.</div>
</section>
<section>
<h2 id="expected-outcomes">Expected outcomes</h2>
<p>The COVID-19 pandemic should alter the relationship between negative memory bias and future BDI scores (<a href="#fig5">Figure 5</a>A). In the study by <a href="#bib9">Zhang et al. (2021)</a>, the percentage of negative words recalled were significantly negatively correlated with future BDI (r = −0.63, p &lt; 0.001) or change in BDI (r = −0.50, p &lt; 0.01) in the DG. And the percentage of negative words recalled was still significantly negatively correlated with future BDI after controlling the effect of current BDI (r = −0.64, p &lt; 0.001, partial correlation). The COVID-19 pandemic should also alter the relationship between response to sad faces in DCC, which is correlated with negative memory bias, and future BDI scores (<a href="#fig5">Figure 5</a>B), as described in <a href="#bib9">Zhang et al. (2021)</a>. Divide the DG into subgroups according to their degree of change in socializing.</p>
<figure id="fig5"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-Fig5.jpg" alt="Fig5.jpg">
<figcaption>
<div class="figcaption-title">Figure 5. Expected relationships between negative memory bias and future BDI in the DG</div>
<p>(A) Future BDI scores or change in BDI scores are plotted against the percentage of negative words recalled in the IFR task.</p>
<p>(B) Future BDI scores or change in BDI scores are plotted against the response to sad faces in bilateral DCC. Figure reprinted with permission from <a href="#bib9">Zhang et al. (2021)</a></p>
</figcaption>
</figure>
</section>
<section>
<h2 id="quantification-and-statistical-analysis">Quantification and statistical analysis</h2>
<h3 id="sec4.1">Statistics of cognitive biases</h3>
<p>The percentage of negative or positive words recalled in the IFR task is used to measure memory bias. Interpretation bias is measured by the pleasantness ratings of the AST-D. The accuracy and reaction time of the sad targets among the neutral distractor conditions in the VST are used to measure attention bias. Pearson correlation analysis is used to measure the relationship between current cognitive biases and future depressive indices.</p>
<h3 id="sec4.2">Statistics of the questions about socializing</h3>
<p>For the questions about social frequency and stress from socializing, the score of the options ranged from 1 to 5. The score of the options for social frequency or stress from socializing during the pandemic were subtracted by the corresponded score before the pandemic and were used as the degree of changes in social frequency or stress from socializing. For the questions about changes in social distance and time spent on socializing during the pandemic, the score of the options ranged from -2 to 2 and were used as the degree of changes in social distance and time spent on socializing. One sample t-tests were used to compare the degree of changes.</p>
<h3 id="sec4.3">Preprocessing of the fMRI data</h3>
<p>The first 5 images of each run should be discarded. The remaining images should be corrected for temporal shifts between slices, realigned, spatially normalized to the MNI space and spatially smoothed (Gaussian kernel, 8 mm full-width at half maximum [FWHM]) using a toolbox for Data Processing &amp; Analysis of Brain Imaging (DPABI) (<a href="#bib8">Yan et al., 2016</a>) based on Statistical Parametric Mapping (SPM12; Welcome Department of Cognitive Neurology, London, United Kingdom, <a href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</a>). To elucidate neural responses that correlated with emotional and neutral faces, a general linear model (GLM) should be used. The regressors of interest are sad face blocks, happy face blocks and neutral face blocks. The regressors of no interest are the fixation blocks. These regressors should be convolved with a hemodynamic response function (HRF) and simultaneously regressed against the blood oxygenation level-dependent (BOLD) signal in each voxel. Six regressors for head motion should also be included.</p>
<h3 id="sec4.4">Participant exclusion criteria</h3>
<p>For the analysis of the IFR task, participants who recalled too few words (the number of words recalled was more than double the standard deviations below the average numbers) were excluded. For the analysis of fMRI data, participants who had poor normalization or high head motion (translation &gt; 2.5 mm or rotation &gt; 2.5°) were excluded.</p>
</section>
<section>
<h2 id="limitations">Limitations</h2>
<p>Although this protocol provides us with the ability to investigate the effect of the COVID-19 pandemic on the relationship between cognitive bias and depression severity in nonclinical depression, it has clear limitations. The grouping criteria were based on the time point of the outbreak of COVID-19. Some of the nonclinical depressed participants’ future depressive indices were assessed during the pandemic, and the others were assessed before the pandemic. Thus, this study cannot be predesigned.</p>
</section>
<section>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="sec6.1">Problem 1</h3>
<p>Future researchers may identify some participants with extremely high depression scores during the participant screening session (step 8 in participant screening section).</p>
<h3 id="sec6.2">Potential solution</h3>
<p>If a participant with extremely high depression score is identified in the participant screening session, researcher can ask a clinician to make a clinical diagnosis for this participant. If the participant is diagnosed with clinical depression, the researcher can suggest the participant to receive clinical treatment and should exclude the participant from the study.</p>
<h3 id="sec6.3">Problem 2</h3>
<p>The accuracy of the participant’s answer is almost 0% or 100% (step 3).</p>
<h3 id="sec6.4">Potential solution</h3>
<p>Sometimes, even though the preset duration in step 3 is the same as the recorded threshold duration in the exercise phase (step 1), participants’ accuracy in step 3 is still too low or too high. This is because participants have not get used to the duration of the words in the exercise phase and the threshold duration is not stable. We recommend repeat the step 1 more times during the exercise phase until a stable threshold duration can be estimated.</p>
<h3 id="sec6.5">Problem 3</h3>
<p>The participant recalls too few words (step 4).</p>
<h3 id="sec6.6">Potential solution</h3>
<p>Give the participant more time to recall. If the number of words recalled is too small (more than double the standard deviations below the average numbers), the data from this participant should be excluded. A possible reason the participant recalls too few words at this step is that they didn’t pay enough attention to the words in step 2. To avoid this issue, the researcher needs to emphasize the importance of attentively watching the words in step2.</p>
<h3 id="sec6.7">Problem 4</h3>
<p>Three tasks were used to assess the participant’s cognitive biases; thus, the participant might have been sleepy during the experiment (steps 2–9).</p>
<h3 id="sec6.8">Potential solution</h3>
<p>Tell the participant to have no less than 7 h of sleep the night before the experiment. Give the participant some time for rest between step 4 and step 5 or step 6 and step 7.</p>
<h3 id="sec6.9">Problem 5</h3>
<p>Participants had excessive head movement during the scan (step 19).</p>
<h3 id="sec6.10">Potential solution</h3>
<p>The experimenter should emphasize to participants the importance of keeping their head still during the scan. Ask the participant to lie on the scanner table in a comfortable position and stabilize the participant’s head via foam padding will also help reduce head movement.</p>
<h3 id="sec6.11">Problem 6</h3>
<p>The task during the fMRI scan does not require responses from participants. Thus some participants may not attentively watch the faces during the scan (step 19).</p>
<h3 id="sec6.12">Potential solution</h3>
<p>Tell participants that there is a camera in the scanner room and the experimenter can watch their performance during the scan. After the end of the scan, ask participants what kinds of faces they have seen in the task to make sure they have attentively watch the faces during the scan.</p>
</section>
<section>
<h2 id="references">References</h2>
<p id="bib1">Bai, L., Ma, H., Huang, Y., and Luo, Y. (2005). The development of native Chinese affective picture system—a pretest in 46 college students. Chin. Ment. Health J. <i>19</i>, 719-722. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref1">View at publisher</a></p>
<p id="bib2">Beck, A.T., Steer, R.A., Ball, R., and Ranieri, W.F. (1996). Comparison of Beck depression inventories -1A and -11 in psychiatric outpatients. J. Pers. Assess. <i>67</i>, 588-597. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref2">View at publisher</a></p>
<p id="bib3">Berna, C., Lang, T.J., Goodwin, G.M., and Holmes, E.A. (2011). Developing a measure of interpretation bias for depressed mood: An ambiguous scenarios test. Pers. Individ. Dif. <i>51</i>, 349-354. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref3">View at publisher</a></p>
<p id="bib4">Kleiner, M., Brainard, D., and Pelli, D. (2007). What's new in Psychtoolbox-3?. Perception <i>36</i>, 14. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref4">View at publisher</a></p>
<p id="bib5">Maldjian, J.A., Laurienti, P.J., Kraft, R.A., and Burdette, J.H. (2003). An automated method for neuroanatomic and cytoarchitectonic atlas-based interrogation of fMRI data sets. Neuroimage <i>19</i>, 1233-1239. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref5">View at publisher</a></p>
<p id="bib6">Tarsia, M., Power, M.J., and Sanavio, E. (2003). Implicit and explicit memory biases in mixed anxiety- depression. J Affect Disord. <i>77</i>, 213-225. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref6">View at publisher</a></p>
<p id="bib7">Wieser, M.J., Hambach, A., and Weymar, M. (2018). Neurophysiological correlates of attentional bias for emotional faces in socially anxious individuals - Evidence from a visual search task and N2pc. Biol. Psychol. <i>132</i>, 192-201. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref7">View at publisher</a></p>
<p id="bib8">Yan, C.G., Wang, X.D., Zuo, X.N., and Zang, Y.F. (2016). DPABI: Data processing &amp; analysis for (resting-state) brain imaging. Neuroinformatics <i>14</i>, 339-351. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref8">View at publisher</a></p>
<p id="bib9">Zhang, P., Piao, Y., Chen, Y., Ren, J., Zhang, L., Qiu, B., Wei, Z., and Zhang, X. (2021). Outbreak of COVID-19 altered the relationship between memory bias and depressive degree in nonclinical depression. iScience <i>24</i>, 102081. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(21)00479-2/sref9">View at publisher</a></p>
</section>
<section>
<h2 id="article-info">Article info</h2>
<h3>Resource availability</h3>
<h4>Lead contact</h4>
<p>Further information and requests for resources should be directed to the lead contact, Xiaochu Zhang (<a href="mailto:zxcustc@ustc.edu.cn">zxcustc@ustc.edu.cn</a>).</p>
<h4>Materials availability</h4>
<p>This study did not generate any new materials.</p>
<h4>Data and code availability</h4>
<p>The MATLAB code for the cognitive bias assessing tasks and stimuli presenting program during fMRI scan are available in <a href="https://github.com/JunbaoZhang/CognitiveBias">https://github.com/JunbaoZhang/CognitiveBias</a>.</p>
<h3>Supplemental information</h3>
<p><a id="mmc1" href="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/957-Mmc1.pdf">Document S1. Socializing questionnaire</a></p>
<h3>Acknowledgments</h3>
<p>We would like to thank the Information Science Center of University of Science and Technology of China for providing access to the GE scanner for the MRI acquisition. This work was supported by grants from the <a href="https://doi.org/10.13039/501100013335">National Key Basic Research Program</a> (2018YFC0831101), the <a href="https://doi.org/10.13039/501100001809">National Natural Science Foundation of China</a> (31771221, 71942003, 61773360, 31800927, 31900766, and 71874170), Major Project of Philosophy and Social Science Research, <a href="https://doi.org/10.13039/501100002338">Ministry of Education of China</a> (19JZD010), CAS-VPST Silk Road Science Fund 2021 (GLHZ202128), Collaborative Innovation Program of Hefei Science Center, and <a href="https://doi.org/10.13039/501100002367">CAS</a> (2020HSC-CIP001). A portion of the numerical calculations in this study were performed with the supercomputing system at the Supercomputing Centre of USTC.</p>
<h3>Author contributions</h3>
<p>P.Z., Z.W., and X.Z. conceived and designed the study. P.Z. obtained findings. P.Z., Y.P., and B.Q. were responsible for acquisition of data. P.Z. and Z.W. analyzed and interpreted the data. Y.C., J.R., and Z.W. provided administrative, technical, or material support. L.Z., Z.W., and X.Z. supervised the study. P.Z. drafted the paper. Z.W. and X.Z. contributed to critical revision for intellectual content.</p>
<h3>Declaration of interests</h3>
<p>The authors declare no competing interests.</p>
</section>
</article>