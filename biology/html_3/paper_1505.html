<article>
<section>
<h1 id="header">Speech-to-Speech Synchronization protocol to classify human participants as high or low auditory-motor synchronizers</h1>
<p><time datetime="2022-03-18">Published: March 18, 2022</time></p>
<p>Fernando Lizcano-Cortés,<sup><a href="#aff1">1</a>,<a href="#fn1">7</a></sup> Ireri Gómez-Varela,<sup><a href="#aff1">1</a>,<a href="#fn1">7</a></sup> Cecilia Mares,<sup><a href="#aff1">1</a>,<a href="#fn1">7</a>,<a href="#fn2">8</a>,<a href="#cor1">*</a></sup> Pascal Wallisch,<sup><a href="#aff2">2</a></sup> Joan Orpella,<sup><a href="#aff2">2</a></sup> David Poeppel,<sup><a href="#aff2">2</a>,<a href="#aff3">3</a>,<a href="#aff4">4</a>,<a href="#aff5">5</a></sup> Pablo Ripollés,<sup><a href="#aff2">2</a>,<a href="#aff4">4</a>,<a href="#aff5">5</a>,<a href="#aff6">6</a></sup> and M. Florencia Assaneo<sup><a href="#aff1">1</a>,<a href="#fn2">8</a>,<a href="#fn3">9</a>,<a href="#cor2">**</a></sup></p>
<p id="aff1"><sup>1</sup>Institute of Neurobiology, UNAM, Querétaro 76230, México</p>
<p id="aff2"><sup>2</sup>Department of Psychology, New York University, New York, NY 10003, USA</p>
<p id="aff3"><sup>3</sup>Ernst Struengmann Institute for Neuroscience, 60528 Frankfurt, Germany</p>
<p id="aff4"><sup>4</sup>Center for Language, Music and Emotion (CLaME), New York University, New York, NY, USA</p>
<p id="aff5"><sup>5</sup>Max Plank Institute for Empirical Aesthetics, 60322 Frankfurt, Germany</p>
<p id="aff6"><sup>6</sup>Music and Audio Research Laboratory (MARL), New York University, New York, NY 11201, USA</p>
<p id="fn1"><sup>7</sup>These authors contributed equally</p>
<p id="fn2"><sup>8</sup>Technical contact</p>
<p id="fn3"><sup>9</sup>Lead contact</p>
<p id="cor1"><sup>*</sup>Correspondence: <a href="mailto:ceciliap.maresr@gmail.com">ceciliap.maresr@gmail.com</a></p>
<p id="cor2"><sup>**</sup>Correspondence: <a href="mailto:fassaneo@inb.unam.mx">fassaneo@inb.unam.mx</a></p>
<p><span class="open-access">Open Access</span> • DOI: <a href="https://doi.org/10.1016/j.xpro.2022.101248">10.1016/j.xpro.2022.101248</a></p>
</section>
<section>
<h2 id="summary">Summary</h2>
<p>The ability to synchronize a motor action to a rhythmic auditory stimulus is often considered an innate human skill. However, some individuals lack the ability to synchronize speech to a perceived syllabic rate. Here, we describe a simple and fast protocol to classify a single native English speaker as being or not being a speech synchronizer. This protocol consists of four parts: the pretest instructions and volume adjustment, the training procedure, the execution of the main task, and data analysis.</p>
<p>For complete details on the use and execution of this protocol, please refer to <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref1">Assaneo et al. (2019a)</a>.</p>




<div class="highlights">
<h3>Highlights</h3>
<ul>
<li>Behavioral protocol to assess individuals’ degree of speech auditory motor synchrony</li>
<li>Individuals can be labeled as high or low speech synchronizers</li>
<li>When assessed over time, classifications were stable</li>
<li>Differences between groups have been shown at brain and cognitive levels</li>
</ul>
</div>
<div class="graphical-abstract">
<h3>Graphical abstract</h3>
<figure><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1505-GA.jpg" alt="GraphicalAbstract.jpg"></figure>
</div></section>
<section>
<h2 id="before-you-begin">Before you begin</h2>
<p>A recent study introduced the Speech-to-Speech Synchronization test, a behavioral protocol showing that the general population can be separated into two groups according to individual differences in the degree of speech auditory-motor synchronization (<a href="#bib1">Assaneo et al., 2019a</a>). Such a binary classification has been evidenced in the study by the bimodal nature of the distribution of the obtained synchronization measurements. Specifically, this study showed that when individuals listen to a rhythmic train of syllables and - concurrently and continuously - whisper the syllable “tah”, some speakers spontaneously align their produced syllabic rate to the perceived one (high synchrony group), while others do not (low synchrony group). Importantly, synchronization measurement remains stable while assessed in two sessions separated by a month (n = 34, Spearman correlation between sessions, r = 0.78, P &lt; 0.001; see <a href="#bib1">Assaneo et al., 2019a</a>). This result implies that group belonging represents a stable individual feature. This work, along with a set of follow up studies, showed that group membership (i.e., being a high or a low synchronizer) is predictive of performance in a set of cognitive tasks (e.g., word learning) as well as of brain structural and functional features (<a href="#bib1">Assaneo et al., 2019a</a>, <a href="#bib2">2019b</a>; <a href="#bib8">Kern et al., 2021</a>). Furthermore, the inclusion of the different synchrony groups in the analysis of experimental outcomes has resulted in the emergence of relevant results that would have been masked by pooling together all individuals in the sample (<a href="#bib3">Assaneo et al., 2020</a>, <a href="#bib4">2021</a>). In that vein, the Speech-to-Speech Synchronization test appears as a robust and easy to administer behavioral tool to enable the inclusion of relevant individual differences into experimental protocols.</p>
<p>Currently there are two different versions of the test, which have been employed in different studies. First, the original <i>Implicit Fixed Version</i>, in which the external (i.e., auditorily presented) syllabic rate remains stable at 4.5 Hz, and participants are not explicitly instructed to synchronize their vocalizations to the auditory stimulus. Second, the <i>Explicit Accelerated Version</i>, in which the external syllabic rate starts at 4.3 Hz and increases in steps of 0.1 Hz every 10 s until it reaches 4.7 Hz, and the participants are explicitly instructed to synchronize their speech output to the perceived speech rate. In this accelerated case, the spontaneous nature of the synchrony relies on the fact that, although participants cannot detect the 0.1 Hz increments in the external syllabic rate, high synchronizers still automatically adjust their spoken pace to the subtly accelerating speech input.</p>
<p>Both versions result in a bimodal distribution of the synchrony measurements (<a href="#fig1">Figure 1</a>) and have been repeatedly used in previous research. Although distinctive brain features between synchrony groups (structural MRI, MEG) were only assessed using the original version (<a href="#bib2">Assaneo et al., 2019b</a>, <a href="#bib3">2020</a>), differences in behavior have been reported with both test versions, with the <i>Implicit Fixed</i> (<a href="#bib1">Assaneo et al., 2019a</a>, <a href="#bib3">2020</a>, <a href="#bib4">2021</a>) as well as with the <i>Explicit Accelerated</i> versions (<a href="#bib8">Kern et al., 2021</a>).The protocol outlined here describes the specific steps required to classify a given native English speaker as a low or a high synchronizer by means of the two existing versions of the Speech-to-Speech Synchronization test. The selection of one of the two alternative test versions is left to the researcher and to considerations about the potential effects of steady rhythmic syllabic trains on the rest of the tasks in a researcher’s protocol.</p>
<figure id="fig1"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1505-Fig1.jpg" alt="Fig1.jpg">
<figcaption>
<div class="figcaption-title">Figure 1. Bimodal distributions produced by the two versions of the Speech-to-Speech Synchronization test</div>
<p>Upper panels: Histograms for the synchronization measurements (Phase Locking Values) obtained by using both versions of the test. Implicit Fixed version on the left (N=255). Explicit Accelerated version on the right (N=190). The colored traces represent the two normal distributions obtained by adjusting a two component gaussian mixture model on the data (Implicit Fixed: Component 1, High Synchronizers; mixing proportion: 0.60, mean: 0.58. Component 2, Low Synchronizers; mixing proportion: 0.40, mean: 0.23. Explicit Accelerated: Component 1, High Synchronizers; mixing proportion: 0.67, mean: 0.63. Component 2, Low Synchronizers; mixing proportion: 0.33, mean: 0.27). Lower panels: Probability of belonging to one of the two groups as a function of the participant’s degree of synchrony. Probability curves are derived from the distributions obtained from the gaussian mixture models adjusted to the datasets. In all panels, orange and blue represent the high and low synchronizers, respectively.</p>
</figcaption>
</figure>
<p>All protocols were reviewed and approved by the local institutional review board, New York University’s Committee on Activities Involving Human Subjects.</p>
</section>
<section>
<h2 id="key-resources-table">Key resources table</h2>
<table id="krt">
<thead>
<tr>
<th>REAGENT or RESOURCE</th>
<th>SOURCE</th>
<th>IDENTIFIER</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Deposited data</td>
</tr>
<tr>
<td>Auditory stimuli, wav files</td>
<td>This paper</td>
<td><a href="https://zenodo.org/badge/latestdoi/407612860">https://zenodo.org/badge/latestdoi/407612860</a></td>
</tr>
<tr>
<td>MATLAB code for running both versions of the test and analyzing its outcome</td>
<td>This paper</td>
<td><a href="https://zenodo.org/badge/latestdoi/407612860">https://zenodo.org/badge/latestdoi/407612860</a></td>
</tr>
<tr>
<td>Python code analyzing the outcome</td>
<td>This paper</td>
<td><a href="https://doi.org/10.5281/zenodo.6148008">https://doi.org/10.5281/zenodo.6148008</a></td>
</tr>
<tr>
<td>Gorilla open materials to run both versions of the test remotely</td>
<td>This paper</td>
<td><a href="https://app.gorilla.sc/openmaterials/290032">https://app.gorilla.sc/openmaterials/290032</a></td>
</tr>
<tr>
<td colspan="3">Experimental models: Organisms/strains</td>
</tr>
<tr>
<td>255 Human participants (112 males; mean age, 30 years; age range, 19–55 years; native English speakers)</td>
<td>(<a href="#bib2">Assaneo et al., 2019b</a>)</td>
<td>N/A</td>
</tr>
<tr>
<td>190 Human participants (81 males; mean age, 25 years; age range, 19–45 years; native English speakers)</td>
<td>This paper</td>
<td>N/A</td>
</tr>
<tr>
<td colspan="3">Software and algorithms</td>
</tr>
<tr>
<td>MATLAB; Version: 9.10.0.1739362 (R2021a)</td>
<td>MathWorks</td>
<td><a href="https://www.mathworks.com">https://www.mathworks.com</a></td>
</tr>
<tr>
<td>Psychtoolbox v3.0.17</td>
<td>(<a href="#bib14">Assaneo et al. (2019a)</a>)</td>
<td><a href="http://psychtoolbox.org">http://psychtoolbox.org</a></td>
</tr>
<tr>
<td>Gorilla Experiment Builder</td>
<td>(<a href="#bib15">Anwyl-Irvine et al., 2020</a>)</td>
<td><a href="https://gorilla.sc">https://gorilla.sc</a></td>
</tr>
<tr>
<td>Praat</td>
<td>(<a href="#bib5">Boersma and Weenink, 2001</a>)</td>
<td><a href="https://praat.en.softonic.com/">https://praat.en.softonic.com/</a></td>
</tr>
</tbody>
</table>
</section>
<section>
<h2 id="step-by-step-method-details">Step-by-step method details</h2>
<p>We focus on two versions of the Speech-to-Speech Synchronization test: the <i>Implicit Fixed</i> and the <i>Explicit Accelerated</i>. Below we detail the steps necessary to carry out both versions. When neither version is mentioned, the particular step does not differ between them. Both versions can be conducted in-lab or remotely/online. The in-lab experimental scripts, as well as the wav files described below are available at <a href="https://zenodo.org/badge/latestdoi/407612860">https://zenodo.org/badge/latestdoi/407612860</a> (MATLAB version) whereas the online scripts to conduct the experiment can be found at <a href="https://app.gorilla.sc/openmaterials/290032">https://app.gorilla.sc/openmaterials/290032</a>.</p>
<p>For the general setup of the experiment, participants should sit in front of a computer close to a microphone and wear headphones. Wearing headphones represents a crucial aspect of the design, for two main reasons: (i) recording should not be contaminated by the auditory stimulus and (ii) participants own voice feedback should be masked by the external stimulus (i.e., participants should not be able to listen to their own vocal production). The microphone can be externally connected to the computer; the computer’s internal microphone can also be used. Participants are instructed to maintain a short distance (below 30 cm) between their mouth and the microphone throughout the test. When the test is conducted in-lab, instructions are given verbally at the start of the experiment and appear written on the computer screen before the beginning of each step.</p>
<h3 id="sec2.1">Part one</h3>
<h4 id="sec2.1.1">Volume adjustment</h4>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 2 min (15 s of volume adjusting + 1.5 min for instructing the participant)</div>
<div class="note">
<span class="note-title">Note:</span> In case of applying the test remotely/online, two additional steps are added to ensure that the participant is wearing headphones and that the microphone is working properly (<a href="#bib12">Woods et al., 2017</a>).</div>
<ol>
<li>Have participants listen to a train of synthesized syllables played backwards (i.e., the same audio wav used as stimulus in the main task, made of 16 syllables randomly concatenated, but reversed in time) while asking them to whisper the syllable “tah”.</li>
<li>Ask participants to gradually increase the volume until they cannot hear their own whisper while still being at a comfortable level.</li>
<li>Once they select the volume level, instruct them not to change the volume throughout the task.</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> The maximal volume reached by the used device and stimuli in our case was 100 dB, stated by the WHO as a safe sound level if listened for 15 min each day (<a href="#bib13">World Health Organization, 2015</a>).</div>
<div class="critical">
<span class="critical-title">Critical:</span> Crucially, all included participants should report not hearing their own voice.</div>
<div class="note">
<span class="note-title">Note:</span> The volume selected by the participants does not distinguish between high and low synchronizers (Mann–Whitney–Wilcoxon test, two-sided P = 0.69; highs: n = 15, meanVol = 95.16 dB, s.d. = 3.84 dB, and lows: n = 16 meanVol = 95.67 dB, s.d. = 4.07 dB).</div>
<h3 id="sec2.2">Part two</h3>
<h4 id="sec2.2.1">Training</h4>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 30 s</div>
<ol start="4">
<li>Have participants passively listen to a 10-s rhythmic train of syllables.</li>
<li>Syllables are presented at 4.5 Hz for the Implicit Fixed Version and 4.3 Hz for the Accelerated Explicit Version (example_45 Hz.wav and example_43 Hz.wav, respectively).</li>
<li>Once the rhythmic train ends, ask participants to whisper the syllable “tah” at the same pace for 10 s.</li>
</ol>
<div class="critical">
<span class="critical-title">Critical:</span> In the implicit version tell participants that this step is for them to get an idea of how they are supposed to be whispering continuously during the main task. More precisely, give the following instructions: “First, you will be presented with an example audio of how to continuously and repeatedly whisper the syllable 'tah.' Pay attention to it and once it ends it will be your turn to practice the whispering.”</div>
<h3 id="sec2.3">Part three</h3>
<h4 id="sec2.3.1">Main task</h4>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 1.5 min (60 s for the main task run + 30 s for the two-alternative forced choice questions only in the Implicit Fixed Version)</div>
<ol start="7">
<li>For the <i>Implicit Fixed</i> version, ask participants to pay attention to the perceived syllables while continuously whispering the syllable ‘tah’. Explain to them that after the presentation, they will be required to identify a subset of the presented syllables and that the point of the continuous ‘tah’ whispering is to make more challenging the syllables recall. Do not disclose <i>that the objective of the whispering is to measure their speech-to-speech synchrony.</i> For the Explicit Accelerated Version, ask participants to synchronize the repeated whispered syllable to the rate of the auditory stimulus.</li>
<li>Have participants listen to a 60-s audio comprising a rhythmic train of syllables (<i>stimulus_fix.wav</i> for the Implicit Fixed version, <i>stimuls_acc.wav</i> for the Explicit Accelerated).</li>
<li>Have participants continuously whisper the syllable “tah” while looking at a fixation cross in the center of the screen during the whole audio presentation and record participants' vocalizations.</li>
<li>Only for the Implicit Fixed Version, once the audio presentation ends. Ask participants to answer four two-alternative forced choice questions about whether a particular syllable was presented or not (e.g., “Did you hear the syllable /bah/?”). Have them respond with the keyboard by pressing Y for yes or N for no.</li>
</ol>
<div class="note">
<span class="note-title">Note:</span> The purpose of this assessment is to direct the participant’s attention to the syllable detection task and to avoid having them intentionally synchronizing their whisper to the auditory stimulus. There is no useful information in the participants’ responses, it has been shown that lows and high synchronizers have equal poor performance on this task (<a href="#bib1">Assaneo et al., 2019a</a>).</div>
<div class="note">
<span class="note-title">Note:</span> The test consists of two runs, therefore Part two and three are repeated.</div>
<h3 id="sec2.4">Part four</h3>
<h4 id="sec2.4.1">Analysis</h4>
<div class="timing">
<span class="timing-title">Timing:</span> approximately 3 min</div>
<ol start="11">
<li>Visualize and listen to the recorded audio signals. We suggest using the software <i>praat</i> (<a href="#bib5">Boersma and Weenink, 2001</a>). If none of the exclusion criteria are reached (see below for details regarding exclusions), each participant is labeled as a high or low synchronizer, following the analysis described below.</li>
</ol>
</section>
<section>
<h2 id="expected-outcomes">Expected outcomes</h2>
<p>For a single participant, the outcome comprises two audio files of 60 s each. Importantly, the recorded acoustic signal should not contain background noise loud enough to mask the reconstruction of the produced speech signal (see <a href="#fig2">Figure 2</a>). Also, researchers should listen to and visualize the audio signal to control for all other exclusion criteria detailed in the following section. If a large sample study (N&gt;60) is conducted, a bimodal distribution is expected for the synchronization measures (see <a href="#fig1">Figure 1</a> for the expected outcome: see troubleshooting, <a href="#sec6.1">problem 1</a> for a possible solution if the bimodal is not clear).</p>
<figure id="fig2"><img src="https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1505-Fig2.jpg" alt="Fig2.jpg">
<figcaption>
<div class="figcaption-title">Figure 2. Examples of a bad and a good audio recording</div>
<p>The upper panel show two schematic outcomes, which are composed of the acoustic signals represented in the middle and bottom panels. In both cases, the participant produced the same train of “tahs” (two bottom rows) and it is the background noise that differs between the right and the left examples. In the example on the left, the audio was recorded with a stable and relatively low background noise, which did not alter the whisper’s envelope. In the example on the right, the background noise shows abrupt increments in amplitude (which could represent different naturalistic sounds such as other voices, a dog’s barking, or a telephone ringing). In this case, as shown in the upper panel, the envelope of the recording does not recover the one of the participant’s whispers. In all panels, the acoustic signal is depicted in gray while the corresponding envelope is highlighted in purple.</p>
</figcaption>
</figure>
</section>
<section>
<h2 id="quantification-and-statistical-analysis">Quantification and statistical analysis</h2>
<p>Before estimating the individual’s degree of speech-to-speech synchrony the audio signal should be evaluated according to the following exclusion criteria. The data should be excluded if: (i) the participant speaks aloud (i.e., activating the vocal cords) instead of whispering, (ii) the recording shows high environmental noise, masking the reconstruction of the speech envelope (see <a href="#fig2">Figure 2</a>), (iii) silence gaps between “tah” utterances are longer than 3 s , (iv) the spoken rate is equal to or lower than 2 Hz (i.e., the participant produces 2 “tahs” or less per second) and (v) the recorded audio is contaminated with the leaking stimulus (troubleshooting, <a href="#sec6.3">problem 2</a>, for a possible solution).</p>
<p>If none of the listed issues are present in the recordings, the phase locking value (PLV) between the envelope of the produced and perceived acoustic signal is computed to classify the participant as a high or a low synchronizer. The PLV for the first and second runs of the test are estimated following the steps described in <a href="#bib1">Assaneo et al. (2019a)</a>. For the datasets presented here (<a href="#fig1">Figure 1</a>), we fitted a linear regression with the PLV obtained during the second run as the dependent variable and the PLV of the first run as the independent variable. The results (<i>y</i> = m<i>x</i> + b) showed a strong correlation between runs in both versions, with coefficients (with 95% confidence bounds): (i) Implicit Fixed; m= 0.90 (0.83, 0.97), b= 0.07 (0.04, 0.10); and (ii) Explicit Accelerated; m = 0.99 (0.94, 1.05), b = 0.01 (-0.02, 0.04). If a given pair of (PLV run1, PLV run2) is outside the 95% confidence bounds of the fitted lines, the participant will be labeled as inconsistent and excluded. Otherwise, the PLV, averaged across runs, will be assigned as the individual’s degree of synchrony. This value will be used to estimate the participant’s probability of being a high (or a low) synchronizer. For this purpose, we fit a gaussian mixture model with two components to the distribution of the synchrony measurements obtained with each version of the test (N=255 for the Implicit Fixed; N=190 for the Explicit Accelerated). The obtained distributions for each component, which represent the low and high synchronizers, have been used to compute a probability function for the individual degrees of synchrony to belong to one or other group (see <a href="#fig1">Figure 1</a>). Given the features of the sample used to compute the probability distribution (i.e., large n and unrestricted participation requirements other than being between 18 and 50 years old) it represents a good approximation of the general adult American population synchronization properties.</p>
<p>A Matlab script implementing the complete analysis pipeline can be downloaded from <a href="https://zenodo.org/badge/latestdoi/407612860">https://zenodo.org/badge/latestdoi/407612860</a> (or from <a href="https://doi.org/10.5281/zenodo.6148008">https://doi.org/10.5281/zenodo.6148008</a> implemented in python). In this script, the researcher should specify the name of the recorded wav files and the version of the test employed. Specifically, the script will perform the following steps: (1) extract the envelope of the produced speech signal and filter it around the stimulus syllabic rate; (2) compute the PLV between the produced and perceived filtered envelopes in windows of 5 s length with an overlap of 2 s; (3) average the PLVs for each audio file (i.e., for run1 and run2); (4) control for consistency between runs; (5) give the probability for the participant being a high or a low synchronizer, according to the distributions presented in this manuscript (see <a href="#fig1">Figure 1</a> and previous paragraph). For more detail about steps (1), (2) and (3) refer to (<a href="#bib1">Assaneo et al., 2019a</a>).</p>
</section>
<section>
<h2 id="limitations">Limitations</h2>
<p>Although both versions of the test have repeatedly produced bimodal distributions when assessing English (<a href="#bib1">Assaneo et al., 2019a</a>) and German (<a href="#bib4">Assaneo et al., 2021</a>) speakers, the protocol still needs to be validated for other languages. While differences in behavior have been reported applying the Explicit Accelerated version, brain differences between groups, as defined by this version, remain unexplored. Further research is required to assess which of the two existing versions splits the population into groups with sharper distinctions.</p>
<p>In addition, the remote/online application of the test yields a high attrition rate: approximately 30%–40% of participants are excluded (in-lab: &lt; 10%). Data are discarded for different reasons, in a hierarchical order from the most to the less frequent they are: (i) recordings collected in very noisy environments or while the participant is not wearing headphones, (ii) participants speak loudly instead of whispering, due to the absence of the researcher to correct them during the training, (iii) technical issues related to the participant’s equipment, most notably the microphone,</p>
</section>
<section>
<h2 id="troubleshooting">Troubleshooting</h2>
<p>The Speech-to-Speech Synchronization test is short and straightforward to complete, therefore, when any of the exclusion criteria are found, a possible solution is to repeat the assessment. It is worth noting that, since being a high or a low synchronizer is a stable individual feature, a repetition of the test will not modify the outcome. Stability can be inferred from the fact that: (i) synchronization measurements are stable across different sessions and (ii) participants do not show an improvement in the second run of the test while compared against the first one (see the linear regression fitted to the data in section <a href="#quantification-and-statistical-analysis">quantification and statistical analysis</a>).</p>
<h3 id="sec6.1">Problem 1</h3>
<p>For studies intended to compare brain or behavioral features between high and low synchronizers it could be advantageous to get a large sample with a similar number of participants from each group (<a href="#bib7">Keppel and Wickens, 2004</a>; <a href="#bib11">Rusticus and Lovato, 2019</a>). While there are statistical methods specifically designed to deal with unbalanced samples (i.e., (<a href="#bib10">Parra-Frutos, 2013</a>)), even for single case studies (<a href="#bib6">Crawford and Garthwaite, 2002</a>), it is convenient to get balanced samples when possible. Specifically, if researchers want to use some of the commonly chosen approaches (e.g., ANOVA, linear mixed models or decoding strategies) and want to maximize the statistical power. For this reason, in this section we expose why getting balanced samples could be problematic and a plausible strategy to overcome this issue, if required.</p>
<p>Since the proportion of high and low synchronizers in the general population is unbalanced (see the gaussian mixture models fitted to the data described in <a href="#fig1">Figure 1</a>); the number of highs will generally exceed the number of lows. Furthermore, having musical training increases the odds of being a high synchronizer (<a href="#bib1">Assaneo et al., 2019a</a>), and the standard participants are students who, in general, have some musical training. For these reasons, it can be sometimes complicated to recruit enough low synchronizers (step 11).</p>
<h3 id="sec6.2">Potential solution</h3>
<p>Debriefing the participants about their music experience before including them into the study using, for example, the Gold-MSI (<a href="#bib9">Müllensiefen et al., 2014</a>). Assessing only participants with no musical training increases the chances of getting a balanced number of high and low synchronizers.</p>
<h3 id="sec6.3">Problem 2</h3>
<p>It is typical to find participants who set the volume too loud, which produces headphones' sound leakage contaminating the recordings (step 2). In these cases, when the researcher listens to the recordings (step 11), they will hear not only the participant′s whisper but also the stimulus. The combination of the perceived and produced acoustic signals into the recording leads to an incorrect estimation of the participant′s synchrony.</p>
<h3 id="sec6.4">Potential solution</h3>
<p>If the experiment is conducted in-lab, we recommend using insert earplugs. Specifically, we found that the ER 1 etymotic earplugs (<a href="http://www.etymotic.com">http://www.etymotic.com</a>) show no sound leakage even for very loud volume settings. When collecting data remotely, we suggest using the stimulus filtered with a bandpass between 0 to 3 kHz (filtered audio files are included in both versions shared in the Gorilla platform). Typically, the whisper's spectral content exceeds 3 kHz. Therefore, applying a stopband filter between 0 and 3 kHz to the recording allows for the removal of the stimulus contamination without altering the envelope of the train of “tahs”. Summarizing, to solve this problem, use the filtered stimulus and apply a stopband filter [0 3000] Hz to the recorded audio signals before running the analysis.</p>
<h3 id="sec6.5">Problem 3</h3>
<p>It is important to notice that the test has only been validated for “whispering participants”. It is not clear whether the bimodal will still be evidenced if participants speak aloud. For those reasons, if the recordings show voiced periods (i.e., periods where the vocal folds are active) longer than 3 s this participant should be removed (step 11). Thus, researchers should avoid participants speaking aloud and we noticed that for some individuals, the difference between speaking aloud and whispering is not clear.</p>
<h3 id="sec6.6">Potential solution</h3>
<p>When the test is performed in the lab, the experimenter should check that the participant understands how to whisper before starting the experiment (step 6). More specifically, the researcher should ask the participant to try and ensure that they are really whispering. If the participant is not doing it correctly, a plausible strategy is to instruct them to place one hand on the throat to feel the difference between activating or not the vocal cords. Another is to imagine telling a secret to a friend in a very quiet place while surrounded by many people.</p>
</section>
<section>
<h2 id="references">References</h2>
<p id="bib1">Assaneo, M.F., Ripollés, P., Orpella, J., Lin, W.M., de Diego-Balaguer, R., and Poeppel, D. (2019a). Spontaneous synchronization to speech reveals neural mechanisms facilitating language learning. Nat. Neurosci. <i>22</i>, 627-632. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref1">View at publisher</a></p>
<p id="bib2">Assaneo, M.F., Rimmele, J.M., Orpella, J., Ripollés, P., de Diego-Balaguer, R., and Poeppel, D. (2019b). The lateralization of speech-brain coupling is differentially modulated by intrinsic auditory and top-down mechanisms. Front. Integr. Neurosci. <i>13</i>, 28. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref2">View at publisher</a></p>
<p id="bib15">Anwyl-Irvine, A.L., Massonnié, J., Flitton, A., Kirkham, N., and Evershed, J.K. (2020). Gorilla in our midst: An online behavioral experiment builder. Behav. Res. <i>52</i>, 388-407. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/optwR5RRAKO8c">View at publisher</a></p>
<p id="bib3">Assaneo, M.F., Orpella, J., Ripollés, P., Noejovich, L., López-Barroso, D., Diego-Balaguer, R. de, and Poeppel, D. (2020). Population-level differences in the neural substrates supporting statistical learning. Preprint at bioRxiv, 2020.07.03.187260. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref3">View at publisher</a></p>
<p id="bib4">Assaneo, M.F., Rimmele, J.M., Sanz Perl, Y., and Poeppel, D. (2021). Speaking rhythmically can shape hearing. Nat. Hum. Behav. <i>5</i>, 71-82. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref4">View at publisher</a></p>
<p id="bib5">Boersma, P. and Weenink, D. (2001). PRAAT, a system for doing phonetics by computer. Glot Int. <i>5</i>, 341-345. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref5">View at publisher</a></p>
<p id="bib14">Brainard, D.H. (1997). The Psychophysics Toolbox. Spat. Vis. <i>10</i>, 433-436. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/optXT5O0PaAdc">View at publisher</a></p>
<p id="bib6">Crawford, J.R. and Garthwaite, P.H. (2002). Investigation of the single case in neuropsychology: confidence limits on the abnormality of test scores and test score differences. Neuropsychologia <i>40</i>, 1196-1208. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref6">View at publisher</a></p>
<p id="bib7">Keppel, G. and Wickens, T.D. (2004). Design and Analysis: A Researcher’s Handbook (Pearson College Div). <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref7">View at publisher</a></p>
<p id="bib8">Kern, P., Assaneo, M.F., Endres, D., Poeppel, D., and Rimmele, J.M. (2021). Preferred auditory temporal processing regimes and auditory-motor synchronization. Psychon. Bull. Rev. <i>28</i>, 1860-1873. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref8">View at publisher</a></p>
<p id="bib9">Müllensiefen, D., Gingras, B., Musil, J., and Stewart, L. (2014). The musicality of non-musicians: an index for assessing musical sophistication in the general population. PLoS ONE <i>9</i>, e89642. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref9">View at publisher</a></p>
<p id="bib10">Parra-Frutos, I. (2013). Testing homogeneity of variances with unequal sample sizes. Comput. Stat. <i>28</i>, 1269-1297. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref10">View at publisher</a></p>
<p id="bib11">Rusticus, S. and Lovato, C. (2019). Impact of sample size and variability on the power and type I error rates of equivalence tests: a simulation study. Pract. Assess. Res. Eval. <i>19</i>, 11. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref11">View at publisher</a></p>
<p id="bib12">Woods, K.J.P., Siegel, M.H., Traer, J., and McDermott, J.H. (2017). Headphone screening to facilitate web-based auditory experiments. Atten. Percept. Psychophys. <i>79</i>, 2064-2072. <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref12">View at publisher</a></p>
<p id="bib13">World Health Organization (2015). Make Listening Safe, (No. WHO/NMH/NVI/15.2). <a class="external-link" href="http://refhub.elsevier.com/S2666-1667(22)00128-9/sref13">View at publisher</a></p>
</section>
<section>
<h2 id="article-info">Article info</h2>
<h3>Resource availability</h3>
<h4>Lead contact</h4>
<p>Further information and requests for resources should be directed to and will be fulfilled by the lead contact, M. Florencia Assaneo (<a href="mailto:fassaneo@inb.unam.mx">fassaneo@inb.unam.mx</a>).</p>
<h4>Materials availability</h4>
<p>Ethical restrictions apply to the original data set comprising individuals’ voices, and it cannot be shared on a public server. All relevant measurements extracted from the original data to construct <a href="#fig1">Figure 1</a> are available from the lead contact author upon request.</p>
<h4>Data and code availability</h4>
<p>The scripts to run both versions of the Speech-to-Speech Synchronization remotely are available at <a href="https://app.gorilla.sc/openmaterials/290032">https://app.gorilla.sc/openmaterials/290032</a>. The scripts to run the tests in-lab and to analyze the data can be found at <a href="https://zenodo.org/badge/latestdoi/407612860">https://zenodo.org/badge/latestdoi/407612860</a> (MATLAB version) and at <a href="https://doi.org/10.5281/zenodo.6148008">https://doi.org/10.5281/zenodo.6148008</a> (python analysis version).</p>
<h3>Acknowledgments</h3>
<p>This work was supported by <a href="https://doi.org/10.13039/501100005739">UNAM</a>-<a href="https://doi.org/10.13039/501100006087">DGAPA</a>-PAPIIT IA202921 (M.F.A.), IBRO Return Home Fellowship (M.F.A.), and by the <a href="https://doi.org/10.13039/100000001">National Science Foundation</a> under grant 2043717 (D.P., P.R., and M.F.A). F.L.-C., I.G.-V., and C.M. received <a href="https://doi.org/10.13039/501100003141">CONACYT</a> funding (CVU: 779254, 1045881 &amp; 1086447, respectively) from the Mexican government.</p>
<h3>Author contributions</h3>
<p>M.F.A., P.R., and D.P. conceived and supervised the project. P.W., M.F.A., and J.O. conducted the experiments. M.F.A., F.L.-C., I.G.-V., and C.M. analyzed the data. M.F.A. and F.L.-C. wrote the manuscript. All authors read and approved the final version of the manuscript.</p>
<h3>Declaration of interests</h3>
<p>The authors declare no competing interests.</p>
</section>
</article>